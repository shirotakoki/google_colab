{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cvae_score_v1_noconditional_fix1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7QU095PS0Fe","executionInfo":{"status":"ok","timestamp":1626844912637,"user_tz":-540,"elapsed":647,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}},"outputId":"4628fb7f-4799-4ad5-fd91-518e6baf259c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bp056EFxSjHr","executionInfo":{"status":"ok","timestamp":1626844913133,"user_tz":-540,"elapsed":11,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}},"outputId":"de11637e-7308-4c5d-804f-827df628330f"},"source":["%cd /content/drive/My Drive/google_colab/music_cvae/music_cvae_notebook/"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/google_colab/music_cvae/music_cvae_notebook\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"04aOu5t8PeeY","executionInfo":{"status":"ok","timestamp":1626844913133,"user_tz":-540,"elapsed":9,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.utils.data\n","import random as rand\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from torch import FloatStorage, nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","from chord_v2 import chord\n","from blstm_dataset_train import my_blstm_dataset as bdtrain\n","from blstm_dataset_test import my_blstm_dataset as bdtest"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbSceVp5Tvuq","executionInfo":{"status":"ok","timestamp":1626844913133,"user_tz":-540,"elapsed":9,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}},"outputId":"df13243e-21a6-4ca1-fa77-001f16403ac7"},"source":["BATCH_SIZE = 5\n","EPOCHS = 500\n","SEQUENCE = 16\n","LIMIT = 50\n","\n","decoders_initial_size = 32  #decoder input size\n","\n","RANDOM_SEED = 1\n","\n","cuda = not False and torch.cuda.is_available()\n","\n","torch.manual_seed(RANDOM_SEED)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f48a99bdc50>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"0b-O7eGZ17lt","executionInfo":{"status":"ok","timestamp":1626844913134,"user_tz":-540,"elapsed":7,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":[""],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"tw0rKSJDT4Id","executionInfo":{"status":"ok","timestamp":1626844913134,"user_tz":-540,"elapsed":6,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["train = bdtrain()\n","test = bdtrain()\n","train_loader = torch.utils.data.DataLoader(\n","    train, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(\n","    test, batch_size=1, shuffle=False)\n","\n","device = torch.device(\"cuda\" if cuda else \"cpu\")"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckoSY--_T4sF","executionInfo":{"status":"ok","timestamp":1626844913134,"user_tz":-540,"elapsed":6,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["class VAE(nn.Module):\n","    def __init__(self,teacher_forcing = False, scheduled_sampling = False):\n","        super(VAE, self).__init__()\n","\n","        self.latent_dim = 128\n","        self.seq_dim = 16\n","        self.feature_dim = 12 + 2\n","        self.lstm_hidden_dim = 512\n","        self.input_dim = self.seq_dim*self.feature_dim\n","        self.fc0 = nn.Linear(self.input_dim, self.input_dim)\n","        self.fc_lstm1 = nn.LSTM(self.feature_dim ,self.lstm_hidden_dim,batch_first = True,bidirectional = True)\n","        self.fc1 = nn.Linear((16*self.lstm_hidden_dim*2), 150)\n","        self.fc21 = nn.Linear(self.lstm_hidden_dim*2, self.latent_dim)\n","        self.fc22 = nn.Linear(self.lstm_hidden_dim*2, self.latent_dim)\n","        self.fc3 = nn.Linear(self.latent_dim, 150)\n","        #self.fc4 = nn.Linear(150, self.input_dim)\n","        self.decoder = nn.LSTM(self.feature_dim + self.latent_dim,\n","                        self.latent_dim,\n","                        num_layers=1,\n","                        batch_first=True)\n","        #self.fc_lstm2 = nn.LSTM(self.feature_dim ,self.lstm_hidden_dim ,batch_first = True)\n","        #self.fc5 = nn.Linear(16*self.lstm_hidden_dim, self.input_dim)\n","        self.linear = nn.Linear(self.latent_dim, self.feature_dim)\n","        self.teacher_forcing = teacher_forcing\n","        self.scheduled_sampling = scheduled_sampling\n","    \n","    def prob(self, epoch, rate = 4):\n","        return 1/(np.exp((epoch/EPOCHS)*rate))\n","\n","    def coinFlip(self, epoch):\n","        if self.prob(epoch) < rand.random():\n","            return True\n","        else:\n","            return False\n","\n","    def encode(self, x):\n","        # print(x.view(-1, self.input_dim))\n","        h0 = F.relu(self.fc0(x.reshape(-1,self.input_dim)))\n","        h0 = h0.reshape(-1,self.seq_dim,self.feature_dim)\n","        f1_lstm_out, f1_lstm_h = self.fc_lstm1(h0)\n","        #f1_lstm_out = F.relu(f1_lstm_out)\n","        #print(f1_lstm_out.shape)\n","        #print(f1_lstm_h[1].shape)\n","        f1_lstm_h = torch.cat([f1_lstm_h[0][0],f1_lstm_h[0][0]],axis = 1)\n","        \"\"\"\n","        h1 = F.relu(\n","            self.fc1(f1_lstm_out.reshape(-1, 16*self.lstm_hidden_dim*2)))\n","        \"\"\"\n","        #h1 = F.relu(self.fc1(f1_lstm_h))\n","\n","        return self.fc21(f1_lstm_h), self.fc22(f1_lstm_h)\n","\n","    def decode(self, latent,x,epoch):\n","        \"\"\"\n","        h3 = F.relu(\n","            self.fc3(latent))\n","        h4 = F.relu(self.fc4(h3))        \n","        h4 = torch.t(h4).reshape(-1, self.seq_dim, self.feature_dim)\n","        h5, h5_c = self.fc_lstm2(h4)\n","        h5 = h5.reshape(-1,16*self.lstm_hidden_dim)\n","        h5 = self.fc5(h5)\n","        \"\"\"\n","        BATCH_SIZE = x.size()[0]\n","        latent = torch.reshape(latent,(BATCH_SIZE,1,-1))\n","        decoder_hidden = (torch.zeros(1,\n","                                BATCH_SIZE,\n","                                self.latent_dim,\n","                                device=device),\n","                          torch.zeros(1,\n","                                BATCH_SIZE,\n","                                self.latent_dim,\n","                                device=device))\n","        note = torch.rand(BATCH_SIZE, 1, self.feature_dim, device=device)\n","        notes = torch.zeros(BATCH_SIZE,16,self.feature_dim, device=device)\n","        counter = 0\n","\n","        \"\"\"\n","        Todo 入力データの場合分けを行う。(Scheduled Sampling)\n","            データの入力表現が合っているか確認。\n","            ネットワークの一部を切り取って確認。\n","            小さい動く部品から組み立てていく。\n","        \"\"\"\n","        if self.scheduled_sampling:\n","            self.teacher_forcing = self.coinFlip(epoch)\n","\n","        for i in range(SEQUENCE):\n","            #print(latent.shape,note.shape)\n","\n","            e = torch.cat([latent, note], dim=-1)\n","            e = e.view(BATCH_SIZE, 1, -1)\n","\n","            # Generate a single note (for each batch)\n","            note, decoder_hidden = self.decoder(e, decoder_hidden)\n","\n","            aux = self.linear(note)\n","            # aux = torch.relu(aux)\n","            # aux = torch.sigmoid(aux)\n","\n","            notes[:, counter, :] = aux.squeeze()\n","\n","            \"\"\"\n","            note = x[:,counter,:]\n","            note = torch.reshape(note, (BATCH_SIZE,1,self.feature_dim))\n","            \"\"\"\n","            if self.teacher_forcing:\n","                note = x[:,i,:].reshape(BATCH_SIZE,1,-1)\n","            else:\n","                note = aux\n","            #print(note.shape)\n","            counter = counter + 1\n","\n","        #recon_x = h5.reshape(-1, self.seq_dim, self.feature_dim)\n","        #recon_x = F.softmax(recon_x,dim=2)\n","        #print(h5.shape)\n","        notes = F.softmax(notes ,dim=2)\n","\n","        return notes\n","    \n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x, epoch):\n","        x = x.float()\n","\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu,logvar)\n","        return self.decode(z,x,epoch),mu, logvar"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDFTYD3VUBym","executionInfo":{"status":"ok","timestamp":1626844913467,"user_tz":-540,"elapsed":338,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["model = VAE(teacher_forcing=True,scheduled_sampling=True).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=2e-3)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9jlBJBMWRwp","executionInfo":{"status":"ok","timestamp":1626844913468,"user_tz":-540,"elapsed":4,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function(recon_x, x, mu, logvar):\n","    cd = chord()\n","    BCE = F.binary_cross_entropy(\n","        recon_x, x, reduction='sum')\n","\n","    # see Appendix B from VAE paper:\n","    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","    # https://arxiv.org/abs/1312.6114\n","    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","\n","    \"\"\"ConsonanseLoss = F.binary_cross_entropy(cd.loss_exam(recon_x, labels),\n","                                            cd.loss_exam(x[0], labels).float().detach())\"\"\"\n","                              \n","    \"\"\"\n","    ConsonanseLoss = torch.zeros(1, requires_grad=True)\n","\n","    for i, (melody_i, chord_i, chord_type_i, consonance_i) in enumerate(zip(x, chord_d, chord_type, consonance)):\n","        ConsonanseLoss = ConsonanseLoss + (\n","            consonance_i - cd.calc_consonance_loss(melody_i, chord_i, chord_type_i).detach())**2\n","\n","    # ConsonanseLoss = ConsonanseLoss.sum()\n","    ConsonanseLoss = BCE.item()/10*ConsonanseLoss\n","    \"\"\"\n","    #print(BCE)\n","    return BCE + KLD\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxYOxbHWfP1t","executionInfo":{"status":"ok","timestamp":1626844913468,"user_tz":-540,"elapsed":4,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["def show_params(net):\n","    for n in net.parameters():\n","        print(n)\n","\n","def show_weights(net):\n","    print(net.decoder.weight.grad)\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zzYHdsrWXw9","executionInfo":{"status":"ok","timestamp":1626844913469,"user_tz":-540,"elapsed":4,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    train_consonanse_loss = 0\n","    cnt = 1\n","    for batch_idx, (melody, chord_d, chord_type) in enumerate(train_loader):\n","        # print(data.size(), labels.size())\n","        # print(labels)\n","        melody = melody.to(device)\n","        chord_d = chord_d.to(device)\n","        chord_type = chord_type.to(device)\n","        optimizer.zero_grad()\n","        recon_batch ,mu, logvar= model(melody,epoch)\n","        loss = loss_function(\n","            recon_batch, melody, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.item()\n","\n","        #ネットワークパラメータを表示\n","        #show_params(model)\n","        #show_weights(model)\n","\n","        #train_consonanse_loss += consonanse_loss.item()\n","        optimizer.step()\n","\n","        if cnt % (100) == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(melody), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader),\n","                loss.item() / len(melody)))\n","        cnt += 1\n","        if batch_idx*BATCH_SIZE >= LIMIT: \n","            break\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","            epoch, train_loss / len(train_loader.dataset)))\n","    return train_loss/BATCH_SIZE, train_consonanse_loss"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"JoDJ5DGPWaf9","executionInfo":{"status":"ok","timestamp":1626844913469,"user_tz":-540,"elapsed":4,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}}},"source":["def out_test():\n","    model.eval()\n","    model.teacher_forcing = False\n","    model.scheduled_sampling = False\n","    test_loss = 0\n","    label_flag = True\n","    with torch.no_grad():\n","        for batch_idx, (melody, chord_d, chord_type) in enumerate(test_loader):\n","            # print(data.size(), labels.size())\n","            # print(labels)\n","            melody = melody.to(device)\n","            chord_d = chord_d.to(device)\n","            chord_type = chord_type.to(device)\n","            optimizer.zero_grad()\n","            mu, logvar = model.encode(melody)\n","            z = model.reparameterize(mu, logvar)\n","            output = model.decode(z,melody,0)\n","\n","            melody = melody.to('cpu').detach().numpy().copy()\n","            output = output.to('cpu').detach().numpy().copy()\n","            melody_1 = pd.DataFrame(\n","                melody[0],\n","                columns=['c', 'c#', 'd', 'd#', 'e', 'f', 'f#', 'g', 'g#', 'a', 'a#', 'b', 'rest','hold'])\n","            output_1 = pd.DataFrame(\n","                output[0],\n","                columns=['c', 'c#', 'd', 'd#', 'e', 'f', 'f#', 'g', 'g#', 'a', 'a#', 'b', 'rest','hold'])\n","            print(z)\n","            print(melody_1)\n","            print(output_1.round(1))\n","            #output.to_csv('test_out.csv')\n","            return melody_1.values,output_1.round(1).values"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WGbQ-s79WcyE","executionInfo":{"status":"ok","timestamp":1626845761616,"user_tz":-540,"elapsed":848151,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}},"outputId":"986fc0bc-bf22-476d-c92b-58e88887c1be"},"source":["if __name__ == \"__main__\":\n","    train_flag = True\n","    model_path = 'models/lstm_cvae_16_noc_f.pth'\n","\n","    loss_array = np.ndarray(0)\n","    consonanse_array = np.ndarray(0)\n","    epoch_array = np.arange(1, EPOCHS+1)\n","\n","    if train_flag:\n","        for epoch in range(1, EPOCHS + 1):\n","            train_loss, train_consonanse_loss = train(epoch)\n","            loss_array = np.append(loss_array, train_loss)\n","            # consonanse_array = np.append(consonanse_array, train_consonanse_loss)\n","            # test(epoch)\n","        torch.save(model.to('cpu').state_dict(), model_path)\n","        plt.plot(np.arange(len(loss_array)),loss_array)\n","        # plt.ylim(0,1000)\n","        plt.show()\n","    else:\n","        model.load_state_dict(torch.load(model_path))\n","        out_test()"],"execution_count":26,"outputs":[{"output_type":"stream","text":["====> Epoch: 1 Average loss: 0.0860\n","====> Epoch: 2 Average loss: 0.0524\n","====> Epoch: 3 Average loss: 0.0429\n","====> Epoch: 4 Average loss: 0.0446\n","====> Epoch: 5 Average loss: 0.0421\n","====> Epoch: 6 Average loss: 0.0415\n","====> Epoch: 7 Average loss: 0.0396\n","====> Epoch: 8 Average loss: 0.0391\n","====> Epoch: 9 Average loss: 0.0415\n","====> Epoch: 10 Average loss: 0.0390\n","====> Epoch: 11 Average loss: 0.0386\n","====> Epoch: 12 Average loss: 0.0396\n","====> Epoch: 13 Average loss: 0.0403\n","====> Epoch: 14 Average loss: 0.0371\n","====> Epoch: 15 Average loss: 0.0373\n","====> Epoch: 16 Average loss: 0.0382\n","====> Epoch: 17 Average loss: 0.0367\n","====> Epoch: 18 Average loss: 0.0366\n","====> Epoch: 19 Average loss: 0.0382\n","====> Epoch: 20 Average loss: 0.0370\n","====> Epoch: 21 Average loss: 0.0370\n","====> Epoch: 22 Average loss: 0.0365\n","====> Epoch: 23 Average loss: 0.0359\n","====> Epoch: 24 Average loss: 0.0354\n","====> Epoch: 25 Average loss: 0.0374\n","====> Epoch: 26 Average loss: 0.0355\n","====> Epoch: 27 Average loss: 0.0353\n","====> Epoch: 28 Average loss: 0.0351\n","====> Epoch: 29 Average loss: 0.0351\n","====> Epoch: 30 Average loss: 0.0351\n","====> Epoch: 31 Average loss: 0.0349\n","====> Epoch: 32 Average loss: 0.0348\n","====> Epoch: 33 Average loss: 0.0349\n","====> Epoch: 34 Average loss: 0.0349\n","====> Epoch: 35 Average loss: 0.0343\n","====> Epoch: 36 Average loss: 0.0346\n","====> Epoch: 37 Average loss: 0.0348\n","====> Epoch: 38 Average loss: 0.0341\n","====> Epoch: 39 Average loss: 0.0343\n","====> Epoch: 40 Average loss: 0.0355\n","====> Epoch: 41 Average loss: 0.0339\n","====> Epoch: 42 Average loss: 0.0323\n","====> Epoch: 43 Average loss: 0.0292\n","====> Epoch: 44 Average loss: 0.0283\n","====> Epoch: 45 Average loss: 0.0284\n","====> Epoch: 46 Average loss: 0.0281\n","====> Epoch: 47 Average loss: 0.0279\n","====> Epoch: 48 Average loss: 0.0275\n","====> Epoch: 49 Average loss: 0.0279\n","====> Epoch: 50 Average loss: 0.0277\n","====> Epoch: 51 Average loss: 0.0275\n","====> Epoch: 52 Average loss: 0.0280\n","====> Epoch: 53 Average loss: 0.0277\n","====> Epoch: 54 Average loss: 0.0278\n","====> Epoch: 55 Average loss: 0.0270\n","====> Epoch: 56 Average loss: 0.0279\n","====> Epoch: 57 Average loss: 0.0274\n","====> Epoch: 58 Average loss: 0.0273\n","====> Epoch: 59 Average loss: 0.0276\n","====> Epoch: 60 Average loss: 0.0274\n","====> Epoch: 61 Average loss: 0.0272\n","====> Epoch: 62 Average loss: 0.0278\n","====> Epoch: 63 Average loss: 0.0276\n","====> Epoch: 64 Average loss: 0.0270\n","====> Epoch: 65 Average loss: 0.0273\n","====> Epoch: 66 Average loss: 0.0275\n","====> Epoch: 67 Average loss: 0.0273\n","====> Epoch: 68 Average loss: 0.0270\n","====> Epoch: 69 Average loss: 0.0270\n","====> Epoch: 70 Average loss: 0.0275\n","====> Epoch: 71 Average loss: 0.0271\n","====> Epoch: 72 Average loss: 0.0271\n","====> Epoch: 73 Average loss: 0.0271\n","====> Epoch: 74 Average loss: 0.0273\n","====> Epoch: 75 Average loss: 0.0269\n","====> Epoch: 76 Average loss: 0.0275\n","====> Epoch: 77 Average loss: 0.0269\n","====> Epoch: 78 Average loss: 0.0269\n","====> Epoch: 79 Average loss: 0.0268\n","====> Epoch: 80 Average loss: 0.0275\n","====> Epoch: 81 Average loss: 0.0268\n","====> Epoch: 82 Average loss: 0.0265\n","====> Epoch: 83 Average loss: 0.0269\n","====> Epoch: 84 Average loss: 0.0268\n","====> Epoch: 85 Average loss: 0.0269\n","====> Epoch: 86 Average loss: 0.0267\n","====> Epoch: 87 Average loss: 0.0271\n","====> Epoch: 88 Average loss: 0.0259\n","====> Epoch: 89 Average loss: 0.0264\n","====> Epoch: 90 Average loss: 0.0260\n","====> Epoch: 91 Average loss: 0.0261\n","====> Epoch: 92 Average loss: 0.0260\n","====> Epoch: 93 Average loss: 0.0256\n","====> Epoch: 94 Average loss: 0.0258\n","====> Epoch: 95 Average loss: 0.0256\n","====> Epoch: 96 Average loss: 0.0255\n","====> Epoch: 97 Average loss: 0.0253\n","====> Epoch: 98 Average loss: 0.0255\n","====> Epoch: 99 Average loss: 0.0256\n","====> Epoch: 100 Average loss: 0.0257\n","====> Epoch: 101 Average loss: 0.0257\n","====> Epoch: 102 Average loss: 0.0259\n","====> Epoch: 103 Average loss: 0.0256\n","====> Epoch: 104 Average loss: 0.0257\n","====> Epoch: 105 Average loss: 0.0256\n","====> Epoch: 106 Average loss: 0.0253\n","====> Epoch: 107 Average loss: 0.0255\n","====> Epoch: 108 Average loss: 0.0250\n","====> Epoch: 109 Average loss: 0.0255\n","====> Epoch: 110 Average loss: 0.0254\n","====> Epoch: 111 Average loss: 0.0255\n","====> Epoch: 112 Average loss: 0.0251\n","====> Epoch: 113 Average loss: 0.0250\n","====> Epoch: 114 Average loss: 0.0252\n","====> Epoch: 115 Average loss: 0.0256\n","====> Epoch: 116 Average loss: 0.0255\n","====> Epoch: 117 Average loss: 0.0254\n","====> Epoch: 118 Average loss: 0.0258\n","====> Epoch: 119 Average loss: 0.0251\n","====> Epoch: 120 Average loss: 0.0249\n","====> Epoch: 121 Average loss: 0.0256\n","====> Epoch: 122 Average loss: 0.0254\n","====> Epoch: 123 Average loss: 0.0254\n","====> Epoch: 124 Average loss: 0.0252\n","====> Epoch: 125 Average loss: 0.0244\n","====> Epoch: 126 Average loss: 0.0253\n","====> Epoch: 127 Average loss: 0.0248\n","====> Epoch: 128 Average loss: 0.0255\n","====> Epoch: 129 Average loss: 0.0253\n","====> Epoch: 130 Average loss: 0.0248\n","====> Epoch: 131 Average loss: 0.0255\n","====> Epoch: 132 Average loss: 0.0249\n","====> Epoch: 133 Average loss: 0.0251\n","====> Epoch: 134 Average loss: 0.0253\n","====> Epoch: 135 Average loss: 0.0253\n","====> Epoch: 136 Average loss: 0.0252\n","====> Epoch: 137 Average loss: 0.0251\n","====> Epoch: 138 Average loss: 0.0248\n","====> Epoch: 139 Average loss: 0.0252\n","====> Epoch: 140 Average loss: 0.0246\n","====> Epoch: 141 Average loss: 0.0248\n","====> Epoch: 142 Average loss: 0.0248\n","====> Epoch: 143 Average loss: 0.0252\n","====> Epoch: 144 Average loss: 0.0246\n","====> Epoch: 145 Average loss: 0.0249\n","====> Epoch: 146 Average loss: 0.0245\n","====> Epoch: 147 Average loss: 0.0245\n","====> Epoch: 148 Average loss: 0.0243\n","====> Epoch: 149 Average loss: 0.0249\n","====> Epoch: 150 Average loss: 0.0244\n","====> Epoch: 151 Average loss: 0.0244\n","====> Epoch: 152 Average loss: 0.0254\n","====> Epoch: 153 Average loss: 0.0252\n","====> Epoch: 154 Average loss: 0.0250\n","====> Epoch: 155 Average loss: 0.0252\n","====> Epoch: 156 Average loss: 0.0245\n","====> Epoch: 157 Average loss: 0.0242\n","====> Epoch: 158 Average loss: 0.0245\n","====> Epoch: 159 Average loss: 0.0242\n","====> Epoch: 160 Average loss: 0.0245\n","====> Epoch: 161 Average loss: 0.0245\n","====> Epoch: 162 Average loss: 0.0244\n","====> Epoch: 163 Average loss: 0.0240\n","====> Epoch: 164 Average loss: 0.0250\n","====> Epoch: 165 Average loss: 0.0244\n","====> Epoch: 166 Average loss: 0.0236\n","====> Epoch: 167 Average loss: 0.0244\n","====> Epoch: 168 Average loss: 0.0240\n","====> Epoch: 169 Average loss: 0.0249\n","====> Epoch: 170 Average loss: 0.0243\n","====> Epoch: 171 Average loss: 0.0252\n","====> Epoch: 172 Average loss: 0.0242\n","====> Epoch: 173 Average loss: 0.0236\n","====> Epoch: 174 Average loss: 0.0247\n","====> Epoch: 175 Average loss: 0.0243\n","====> Epoch: 176 Average loss: 0.0246\n","====> Epoch: 177 Average loss: 0.0236\n","====> Epoch: 178 Average loss: 0.0246\n","====> Epoch: 179 Average loss: 0.0236\n","====> Epoch: 180 Average loss: 0.0242\n","====> Epoch: 181 Average loss: 0.0241\n","====> Epoch: 182 Average loss: 0.0242\n","====> Epoch: 183 Average loss: 0.0239\n","====> Epoch: 184 Average loss: 0.0245\n","====> Epoch: 185 Average loss: 0.0241\n","====> Epoch: 186 Average loss: 0.0243\n","====> Epoch: 187 Average loss: 0.0240\n","====> Epoch: 188 Average loss: 0.0243\n","====> Epoch: 189 Average loss: 0.0236\n","====> Epoch: 190 Average loss: 0.0237\n","====> Epoch: 191 Average loss: 0.0249\n","====> Epoch: 192 Average loss: 0.0235\n","====> Epoch: 193 Average loss: 0.0240\n","====> Epoch: 194 Average loss: 0.0233\n","====> Epoch: 195 Average loss: 0.0247\n","====> Epoch: 196 Average loss: 0.0245\n","====> Epoch: 197 Average loss: 0.0236\n","====> Epoch: 198 Average loss: 0.0233\n","====> Epoch: 199 Average loss: 0.0233\n","====> Epoch: 200 Average loss: 0.0236\n","====> Epoch: 201 Average loss: 0.0239\n","====> Epoch: 202 Average loss: 0.0244\n","====> Epoch: 203 Average loss: 0.0243\n","====> Epoch: 204 Average loss: 0.0234\n","====> Epoch: 205 Average loss: 0.0235\n","====> Epoch: 206 Average loss: 0.0246\n","====> Epoch: 207 Average loss: 0.0240\n","====> Epoch: 208 Average loss: 0.0238\n","====> Epoch: 209 Average loss: 0.0249\n","====> Epoch: 210 Average loss: 0.0234\n","====> Epoch: 211 Average loss: 0.0232\n","====> Epoch: 212 Average loss: 0.0238\n","====> Epoch: 213 Average loss: 0.0236\n","====> Epoch: 214 Average loss: 0.0244\n","====> Epoch: 215 Average loss: 0.0231\n","====> Epoch: 216 Average loss: 0.0238\n","====> Epoch: 217 Average loss: 0.0235\n","====> Epoch: 218 Average loss: 0.0235\n","====> Epoch: 219 Average loss: 0.0235\n","====> Epoch: 220 Average loss: 0.0235\n","====> Epoch: 221 Average loss: 0.0231\n","====> Epoch: 222 Average loss: 0.0223\n","====> Epoch: 223 Average loss: 0.0238\n","====> Epoch: 224 Average loss: 0.0241\n","====> Epoch: 225 Average loss: 0.0233\n","====> Epoch: 226 Average loss: 0.0232\n","====> Epoch: 227 Average loss: 0.0232\n","====> Epoch: 228 Average loss: 0.0235\n","====> Epoch: 229 Average loss: 0.0226\n","====> Epoch: 230 Average loss: 0.0231\n","====> Epoch: 231 Average loss: 0.0235\n","====> Epoch: 232 Average loss: 0.0226\n","====> Epoch: 233 Average loss: 0.0221\n","====> Epoch: 234 Average loss: 0.0242\n","====> Epoch: 235 Average loss: 0.0230\n","====> Epoch: 236 Average loss: 0.0229\n","====> Epoch: 237 Average loss: 0.0234\n","====> Epoch: 238 Average loss: 0.0225\n","====> Epoch: 239 Average loss: 0.0230\n","====> Epoch: 240 Average loss: 0.0228\n","====> Epoch: 241 Average loss: 0.0234\n","====> Epoch: 242 Average loss: 0.0222\n","====> Epoch: 243 Average loss: 0.0231\n","====> Epoch: 244 Average loss: 0.0225\n","====> Epoch: 245 Average loss: 0.0226\n","====> Epoch: 246 Average loss: 0.0226\n","====> Epoch: 247 Average loss: 0.0239\n","====> Epoch: 248 Average loss: 0.0222\n","====> Epoch: 249 Average loss: 0.0226\n","====> Epoch: 250 Average loss: 0.0232\n","====> Epoch: 251 Average loss: 0.0224\n","====> Epoch: 252 Average loss: 0.0232\n","====> Epoch: 253 Average loss: 0.0227\n","====> Epoch: 254 Average loss: 0.0233\n","====> Epoch: 255 Average loss: 0.0233\n","====> Epoch: 256 Average loss: 0.0226\n","====> Epoch: 257 Average loss: 0.0224\n","====> Epoch: 258 Average loss: 0.0231\n","====> Epoch: 259 Average loss: 0.0232\n","====> Epoch: 260 Average loss: 0.0220\n","====> Epoch: 261 Average loss: 0.0217\n","====> Epoch: 262 Average loss: 0.0232\n","====> Epoch: 263 Average loss: 0.0225\n","====> Epoch: 264 Average loss: 0.0222\n","====> Epoch: 265 Average loss: 0.0221\n","====> Epoch: 266 Average loss: 0.0219\n","====> Epoch: 267 Average loss: 0.0233\n","====> Epoch: 268 Average loss: 0.0229\n","====> Epoch: 269 Average loss: 0.0223\n","====> Epoch: 270 Average loss: 0.0239\n","====> Epoch: 271 Average loss: 0.0230\n","====> Epoch: 272 Average loss: 0.0228\n","====> Epoch: 273 Average loss: 0.0227\n","====> Epoch: 274 Average loss: 0.0225\n","====> Epoch: 275 Average loss: 0.0212\n","====> Epoch: 276 Average loss: 0.0218\n","====> Epoch: 277 Average loss: 0.0216\n","====> Epoch: 278 Average loss: 0.0226\n","====> Epoch: 279 Average loss: 0.0216\n","====> Epoch: 280 Average loss: 0.0222\n","====> Epoch: 281 Average loss: 0.0209\n","====> Epoch: 282 Average loss: 0.0216\n","====> Epoch: 283 Average loss: 0.0227\n","====> Epoch: 284 Average loss: 0.0215\n","====> Epoch: 285 Average loss: 0.0221\n","====> Epoch: 286 Average loss: 0.0218\n","====> Epoch: 287 Average loss: 0.0214\n","====> Epoch: 288 Average loss: 0.0216\n","====> Epoch: 289 Average loss: 0.0216\n","====> Epoch: 290 Average loss: 0.0215\n","====> Epoch: 291 Average loss: 0.0214\n","====> Epoch: 292 Average loss: 0.0204\n","====> Epoch: 293 Average loss: 0.0218\n","====> Epoch: 294 Average loss: 0.0215\n","====> Epoch: 295 Average loss: 0.0221\n","====> Epoch: 296 Average loss: 0.0213\n","====> Epoch: 297 Average loss: 0.0220\n","====> Epoch: 298 Average loss: 0.0221\n","====> Epoch: 299 Average loss: 0.0217\n","====> Epoch: 300 Average loss: 0.0203\n","====> Epoch: 301 Average loss: 0.0217\n","====> Epoch: 302 Average loss: 0.0216\n","====> Epoch: 303 Average loss: 0.0219\n","====> Epoch: 304 Average loss: 0.0200\n","====> Epoch: 305 Average loss: 0.0218\n","====> Epoch: 306 Average loss: 0.0212\n","====> Epoch: 307 Average loss: 0.0213\n","====> Epoch: 308 Average loss: 0.0219\n","====> Epoch: 309 Average loss: 0.0223\n","====> Epoch: 310 Average loss: 0.0209\n","====> Epoch: 311 Average loss: 0.0217\n","====> Epoch: 312 Average loss: 0.0204\n","====> Epoch: 313 Average loss: 0.0231\n","====> Epoch: 314 Average loss: 0.0222\n","====> Epoch: 315 Average loss: 0.0215\n","====> Epoch: 316 Average loss: 0.0208\n","====> Epoch: 317 Average loss: 0.0199\n","====> Epoch: 318 Average loss: 0.0207\n","====> Epoch: 319 Average loss: 0.0203\n","====> Epoch: 320 Average loss: 0.0205\n","====> Epoch: 321 Average loss: 0.0211\n","====> Epoch: 322 Average loss: 0.0209\n","====> Epoch: 323 Average loss: 0.0210\n","====> Epoch: 324 Average loss: 0.0209\n","====> Epoch: 325 Average loss: 0.0208\n","====> Epoch: 326 Average loss: 0.0195\n","====> Epoch: 327 Average loss: 0.0206\n","====> Epoch: 328 Average loss: 0.0200\n","====> Epoch: 329 Average loss: 0.0205\n","====> Epoch: 330 Average loss: 0.0217\n","====> Epoch: 331 Average loss: 0.0207\n","====> Epoch: 332 Average loss: 0.0212\n","====> Epoch: 333 Average loss: 0.0199\n","====> Epoch: 334 Average loss: 0.0213\n","====> Epoch: 335 Average loss: 0.0211\n","====> Epoch: 336 Average loss: 0.0201\n","====> Epoch: 337 Average loss: 0.0198\n","====> Epoch: 338 Average loss: 0.0205\n","====> Epoch: 339 Average loss: 0.0206\n","====> Epoch: 340 Average loss: 0.0198\n","====> Epoch: 341 Average loss: 0.0202\n","====> Epoch: 342 Average loss: 0.0195\n","====> Epoch: 343 Average loss: 0.0206\n","====> Epoch: 344 Average loss: 0.0192\n","====> Epoch: 345 Average loss: 0.0196\n","====> Epoch: 346 Average loss: 0.0209\n","====> Epoch: 347 Average loss: 0.0196\n","====> Epoch: 348 Average loss: 0.0200\n","====> Epoch: 349 Average loss: 0.0204\n","====> Epoch: 350 Average loss: 0.0188\n","====> Epoch: 351 Average loss: 0.0195\n","====> Epoch: 352 Average loss: 0.0201\n","====> Epoch: 353 Average loss: 0.0200\n","====> Epoch: 354 Average loss: 0.0202\n","====> Epoch: 355 Average loss: 0.0208\n","====> Epoch: 356 Average loss: 0.0202\n","====> Epoch: 357 Average loss: 0.0204\n","====> Epoch: 358 Average loss: 0.0202\n","====> Epoch: 359 Average loss: 0.0198\n","====> Epoch: 360 Average loss: 0.0193\n","====> Epoch: 361 Average loss: 0.0200\n","====> Epoch: 362 Average loss: 0.0198\n","====> Epoch: 363 Average loss: 0.0188\n","====> Epoch: 364 Average loss: 0.0195\n","====> Epoch: 365 Average loss: 0.0190\n","====> Epoch: 366 Average loss: 0.0192\n","====> Epoch: 367 Average loss: 0.0190\n","====> Epoch: 368 Average loss: 0.0194\n","====> Epoch: 369 Average loss: 0.0186\n","====> Epoch: 370 Average loss: 0.0198\n","====> Epoch: 371 Average loss: 0.0201\n","====> Epoch: 372 Average loss: 0.0191\n","====> Epoch: 373 Average loss: 0.0194\n","====> Epoch: 374 Average loss: 0.0201\n","====> Epoch: 375 Average loss: 0.0198\n","====> Epoch: 376 Average loss: 0.0195\n","====> Epoch: 377 Average loss: 0.0192\n","====> Epoch: 378 Average loss: 0.0201\n","====> Epoch: 379 Average loss: 0.0195\n","====> Epoch: 380 Average loss: 0.0205\n","====> Epoch: 381 Average loss: 0.0200\n","====> Epoch: 382 Average loss: 0.0183\n","====> Epoch: 383 Average loss: 0.0195\n","====> Epoch: 384 Average loss: 0.0189\n","====> Epoch: 385 Average loss: 0.0179\n","====> Epoch: 386 Average loss: 0.0196\n","====> Epoch: 387 Average loss: 0.0184\n","====> Epoch: 388 Average loss: 0.0186\n","====> Epoch: 389 Average loss: 0.0182\n","====> Epoch: 390 Average loss: 0.0182\n","====> Epoch: 391 Average loss: 0.0182\n","====> Epoch: 392 Average loss: 0.0183\n","====> Epoch: 393 Average loss: 0.0192\n","====> Epoch: 394 Average loss: 0.0182\n","====> Epoch: 395 Average loss: 0.0191\n","====> Epoch: 396 Average loss: 0.0174\n","====> Epoch: 397 Average loss: 0.0189\n","====> Epoch: 398 Average loss: 0.0184\n","====> Epoch: 399 Average loss: 0.0180\n","====> Epoch: 400 Average loss: 0.0177\n","====> Epoch: 401 Average loss: 0.0179\n","====> Epoch: 402 Average loss: 0.0190\n","====> Epoch: 403 Average loss: 0.0182\n","====> Epoch: 404 Average loss: 0.0182\n","====> Epoch: 405 Average loss: 0.0192\n","====> Epoch: 406 Average loss: 0.0188\n","====> Epoch: 407 Average loss: 0.0182\n","====> Epoch: 408 Average loss: 0.0187\n","====> Epoch: 409 Average loss: 0.0181\n","====> Epoch: 410 Average loss: 0.0190\n","====> Epoch: 411 Average loss: 0.0191\n","====> Epoch: 412 Average loss: 0.0188\n","====> Epoch: 413 Average loss: 0.0190\n","====> Epoch: 414 Average loss: 0.0164\n","====> Epoch: 415 Average loss: 0.0179\n","====> Epoch: 416 Average loss: 0.0178\n","====> Epoch: 417 Average loss: 0.0195\n","====> Epoch: 418 Average loss: 0.0201\n","====> Epoch: 419 Average loss: 0.0176\n","====> Epoch: 420 Average loss: 0.0184\n","====> Epoch: 421 Average loss: 0.0177\n","====> Epoch: 422 Average loss: 0.0170\n","====> Epoch: 423 Average loss: 0.0181\n","====> Epoch: 424 Average loss: 0.0173\n","====> Epoch: 425 Average loss: 0.0184\n","====> Epoch: 426 Average loss: 0.0175\n","====> Epoch: 427 Average loss: 0.0188\n","====> Epoch: 428 Average loss: 0.0169\n","====> Epoch: 429 Average loss: 0.0176\n","====> Epoch: 430 Average loss: 0.0176\n","====> Epoch: 431 Average loss: 0.0171\n","====> Epoch: 432 Average loss: 0.0167\n","====> Epoch: 433 Average loss: 0.0170\n","====> Epoch: 434 Average loss: 0.0166\n","====> Epoch: 435 Average loss: 0.0174\n","====> Epoch: 436 Average loss: 0.0177\n","====> Epoch: 437 Average loss: 0.0165\n","====> Epoch: 438 Average loss: 0.0184\n","====> Epoch: 439 Average loss: 0.0180\n","====> Epoch: 440 Average loss: 0.0173\n","====> Epoch: 441 Average loss: 0.0181\n","====> Epoch: 442 Average loss: 0.0185\n","====> Epoch: 443 Average loss: 0.0181\n","====> Epoch: 444 Average loss: 0.0171\n","====> Epoch: 445 Average loss: 0.0167\n","====> Epoch: 446 Average loss: 0.0169\n","====> Epoch: 447 Average loss: 0.0180\n","====> Epoch: 448 Average loss: 0.0167\n","====> Epoch: 449 Average loss: 0.0177\n","====> Epoch: 450 Average loss: 0.0169\n","====> Epoch: 451 Average loss: 0.0160\n","====> Epoch: 452 Average loss: 0.0171\n","====> Epoch: 453 Average loss: 0.0173\n","====> Epoch: 454 Average loss: 0.0175\n","====> Epoch: 455 Average loss: 0.0161\n","====> Epoch: 456 Average loss: 0.0155\n","====> Epoch: 457 Average loss: 0.0174\n","====> Epoch: 458 Average loss: 0.0179\n","====> Epoch: 459 Average loss: 0.0174\n","====> Epoch: 460 Average loss: 0.0165\n","====> Epoch: 461 Average loss: 0.0174\n","====> Epoch: 462 Average loss: 0.0172\n","====> Epoch: 463 Average loss: 0.0154\n","====> Epoch: 464 Average loss: 0.0166\n","====> Epoch: 465 Average loss: 0.0176\n","====> Epoch: 466 Average loss: 0.0178\n","====> Epoch: 467 Average loss: 0.0158\n","====> Epoch: 468 Average loss: 0.0179\n","====> Epoch: 469 Average loss: 0.0163\n","====> Epoch: 470 Average loss: 0.0161\n","====> Epoch: 471 Average loss: 0.0161\n","====> Epoch: 472 Average loss: 0.0180\n","====> Epoch: 473 Average loss: 0.0158\n","====> Epoch: 474 Average loss: 0.0164\n","====> Epoch: 475 Average loss: 0.0154\n","====> Epoch: 476 Average loss: 0.0174\n","====> Epoch: 477 Average loss: 0.0167\n","====> Epoch: 478 Average loss: 0.0165\n","====> Epoch: 479 Average loss: 0.0157\n","====> Epoch: 480 Average loss: 0.0165\n","====> Epoch: 481 Average loss: 0.0155\n","====> Epoch: 482 Average loss: 0.0165\n","====> Epoch: 483 Average loss: 0.0177\n","====> Epoch: 484 Average loss: 0.0158\n","====> Epoch: 485 Average loss: 0.0154\n","====> Epoch: 486 Average loss: 0.0161\n","====> Epoch: 487 Average loss: 0.0154\n","====> Epoch: 488 Average loss: 0.0161\n","====> Epoch: 489 Average loss: 0.0168\n","====> Epoch: 490 Average loss: 0.0178\n","====> Epoch: 491 Average loss: 0.0172\n","====> Epoch: 492 Average loss: 0.0155\n","====> Epoch: 493 Average loss: 0.0161\n","====> Epoch: 494 Average loss: 0.0155\n","====> Epoch: 495 Average loss: 0.0176\n","====> Epoch: 496 Average loss: 0.0163\n","====> Epoch: 497 Average loss: 0.0167\n","====> Epoch: 498 Average loss: 0.0154\n","====> Epoch: 499 Average loss: 0.0149\n","====> Epoch: 500 Average loss: 0.0165\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"9ulksoiR579D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626845762134,"user_tz":-540,"elapsed":528,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}},"outputId":"5c92eb02-d25f-4838-cdb9-984a36662117"},"source":["model.load_state_dict(torch.load(model_path))\n","input,output = out_test()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["tensor([[ 1.5396, -0.8758,  1.1074, -0.2966,  0.0061, -0.2509, -0.1155, -1.2535,\n","          0.1661, -0.5388,  0.7990,  0.6516, -1.4027,  1.8964,  0.8199,  0.6189,\n","          0.8974,  0.0692,  0.1065,  0.2875,  1.1383,  0.2516, -0.6117, -0.5948,\n","         -0.0273,  0.1869,  0.8242,  0.7250,  0.3195, -0.7804,  1.5591,  1.2626,\n","         -0.8399,  1.8732, -1.3609, -0.4554,  0.7168, -0.6020, -0.5637, -0.1720,\n","         -0.4718,  0.3674, -0.5322,  0.3009,  0.3338,  0.3709,  2.1584, -0.0402,\n","         -1.5899, -0.2496, -0.6637, -0.4828,  2.4916, -0.6738, -1.4846, -0.1270,\n","         -0.1843, -0.1029,  1.3435, -1.3932,  1.9323, -0.8590, -0.5566,  0.1990,\n","         -1.1142, -0.2500,  0.3289, -0.6797, -1.1882,  1.9421,  1.1098, -0.3004,\n","         -0.1564,  1.0685,  1.0166, -0.0683, -0.3033,  1.4578,  1.3294,  0.2413,\n","          1.0934, -0.4436, -0.9252, -1.4645,  0.8105, -0.7925, -0.6178,  1.9658,\n","         -1.6316,  0.0159,  0.3009, -1.7133,  0.8645, -0.9317, -0.4510, -0.8554,\n","         -0.8983, -0.4637, -0.4303, -0.3259,  0.7861, -0.8779, -0.3333, -0.3291,\n","          0.2043,  0.3910, -0.5917, -1.1393, -0.7408,  1.0560,  1.1185,  0.2413,\n","          1.1234, -0.3593,  1.7207, -0.1746, -0.6519,  0.5642,  0.6501, -0.3608,\n","         -0.3559, -1.1214,  0.3746, -0.2024, -0.5059, -1.7645, -0.5553,  0.4414]])\n","      c   c#    d   d#    e    f   f#    g   g#    a   a#    b  rest  hold\n","0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0   0.0\n","1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","4   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0\n","5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   0.0   0.0\n","9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","12  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0\n","13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","      c   c#    d   d#    e    f   f#    g   g#    a   a#    b  rest  hold\n","0   0.1  0.0  0.0  0.0  0.0  0.1  0.0  0.0  0.0  0.0  0.0  0.0   0.7   0.0\n","1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","2   0.0  0.0  0.0  0.0  0.0  0.1  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.8\n","3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","4   0.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.1  0.0  0.0   0.0   0.0\n","5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","6   0.0  0.0  0.0  0.0  0.0  0.1  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.9\n","7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","8   0.4  0.0  0.0  0.0  0.1  0.1  0.0  0.0  0.0  0.3  0.0  0.0   0.0   0.1\n","9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","10  0.0  0.0  0.0  0.0  0.0  0.1  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.9\n","11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","12  0.1  0.0  0.0  0.0  0.0  0.2  0.0  0.0  0.0  0.1  0.0  0.0   0.0   0.7\n","13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n","14  0.2  0.0  0.0  0.0  0.0  0.1  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.6\n","15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALPJS2NDjepj","executionInfo":{"status":"ok","timestamp":1626845762134,"user_tz":-540,"elapsed":5,"user":{"displayName":"teranosinn k","photoUrl":"","userId":"05898131523286450651"}},"outputId":"83e9e8c3-24ad-4f4f-cf24-bc590d4a28ca"},"source":["np.round(100.0*(input.size - np.abs(input - output).sum())/input.size,4)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["97.6339"]},"metadata":{"tags":[]},"execution_count":28}]}]}